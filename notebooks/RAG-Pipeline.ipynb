{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55951954-d846-46f7-bb09-4d3ae4f9f216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported.\n",
      "All directories ensured to exist.\n",
      "PDFs will be read from: E:\\PROJECTS\\climate_legal_readiness_project\\data\\legal_docs\n",
      "Final English text will be in: E:\\PROJECTS\\climate_legal_readiness_project\\data\\processed\\texts_translated\n",
      "Chunks will be saved to: E:\\PROJECTS\\climate_legal_readiness_project\\data\\processed\\chunks\n",
      "Vector DB will be in: E:\\PROJECTS\\climate_legal_readiness_project\\data\\processed\\vectorstore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pdfplumber\n",
    "from PyPDF2 import PdfReader\n",
    "from langdetect import detect\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import ollama\n",
    "\n",
    "print(\"All libraries imported.\")\n",
    "\n",
    "BASE = r\"E:\\PROJECTS\\climate_legal_readiness_project\"\n",
    "DATA_DIR = os.path.join(BASE, \"data\")\n",
    "RAW_PDFS_DIR = os.path.join(DATA_DIR, \"legal_docs\")\n",
    "\n",
    "PROC_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "TEXT_DIR = os.path.join(PROC_DIR, \"texts\")\n",
    "TRANSLATED_DIR = os.path.join(PROC_DIR, \"texts_translated\")\n",
    "CHUNK_DIR = os.path.join(PROC_DIR, \"chunks\")\n",
    "VECTOR_DIR = os.path.join(PROC_DIR, \"vectorstore\")\n",
    "\n",
    "for path in [PROC_DIR, TEXT_DIR, TRANSLATED_DIR, CHUNK_DIR, VECTOR_DIR]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(f\"All directories ensured to exist.\")\n",
    "print(f\"PDFs will be read from: {RAW_PDFS_DIR}\")\n",
    "print(f\"Final English text will be in: {TRANSLATED_DIR}\")\n",
    "print(f\"Chunks will be saved to: {CHUNK_DIR}\")\n",
    "print(f\"Vector DB will be in: {VECTOR_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db088a9c-f427-4649-b2da-535362c87f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    text = text.replace(\"\\x00\", \"\")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text) \n",
    "    text = re.sub(r\"\\n\\s*\\n\\s*\\n+\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "pdf_files = [f for f in os.listdir(RAW_PDFS_DIR) if f.lower().endswith(\".pdf\")]\n",
    "print(f\"Found {len(pdf_files)} PDF files to extract.\")\n",
    "report = []\n",
    "\n",
    "for pdf in tqdm(pdf_files, desc=\"Extracting text from PDFs\"): \n",
    "    in_path = os.path.join(RAW_PDFS_DIR, pdf)\n",
    "    out_path = os.path.join(TEXT_DIR, os.path.splitext(pdf)[0] + \".txt\")\n",
    "    \n",
    "    if os.path.exists(out_path) and os.path.getsize(out_path) > 200:\n",
    "        report.append({\"file\": pdf, \"status\": \"exists\"})\n",
    "        continue\n",
    "\n",
    "    pages = []\n",
    "    try:\n",
    "        with pdfplumber.open(in_path) as doc:\n",
    "            for p in doc.pages:\n",
    "                txt = p.extract_text() or \"\"\n",
    "                pages.append(txt)\n",
    "    except Exception as e:\n",
    "        print(f\"pdfplumber failed for {pdf}: {e}. Trying PyPDF2.\")\n",
    "        pages = []\n",
    "\n",
    "    if sum(len(p) for p in pages) < 50:\n",
    "        try:\n",
    "            r = PdfReader(in_path)\n",
    "            pages = [p.extract_text() or \"\" for p in r.pages]\n",
    "        except Exception as e:\n",
    "            print(f\"PyPDF2 also failed for {pdf}: {e}\")\n",
    "            pages = []\n",
    "\n",
    "    text = basic_clean(\"\\n\\n\".join(pages))\n",
    "    \n",
    "    if len(text) < 50:\n",
    "        status = \"empty_or_scanned_pdf\"\n",
    "    else:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        try:\n",
    "            lang = detect(text[:1000])\n",
    "        except:\n",
    "            lang = \"unknown\"\n",
    "        status = f\"ok ({lang})\"\n",
    "        \n",
    "    report.append({\"file\": pdf, \"status\": status})\n",
    "\n",
    "print(\"\\nPDF extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4803426-bd64-45c6-8881-2285bd31c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_cache = {}\n",
    "def get_translator(model_name):\n",
    "    if model_name not in model_cache:\n",
    "        print(f\"Loading model {model_name}... (This may take a while)\")\n",
    "        model_cache[model_name] = {\n",
    "            \"tokenizer\": MarianTokenizer.from_pretrained(model_name),\n",
    "            \"model\": MarianMTModel.from_pretrained(model_name).to(device)\n",
    "        }\n",
    "    return model_cache[model_name]\n",
    "\n",
    "def translate_text(text, model_name, chunk_size=1000):\n",
    "    translator = get_translator(model_name)\n",
    "    tokenizer = translator[\"tokenizer\"]\n",
    "    model = translator[\"model\"]\n",
    "    \n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    translated_parts = []\n",
    "    \n",
    "    for ch in tqdm(chunks, desc=f\"Translating {model_name}\"):\n",
    "        try:\n",
    "            inputs = tokenizer([ch], return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "            translated_ids = model.generate(**inputs, max_length=512)\n",
    "            translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "            translated_parts.append(translated_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating chunk: {e}\")\n",
    "            translated_parts.append(\"[Translation Error]\") \n",
    "            \n",
    "    return \"\\n\\n\".join(translated_parts)\n",
    "\n",
    "all_text_files = [f for f in os.listdir(TEXT_DIR) if f.endswith(\".txt\") and not f.startswith(\"_\")]\n",
    "\n",
    "for fname in tqdm(all_text_files, desc=\"Processing all text files\"): \n",
    "    in_path = os.path.join(TEXT_DIR, fname)\n",
    "    out_path = os.path.join(TRANSLATED_DIR, fname)\n",
    "    \n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"Skipping {fname}, translated file already exists.\")\n",
    "        continue\n",
    "        \n",
    "    with open(in_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    if len(text) < 50:\n",
    "        print(f\"Skipping short file: {fname}\")\n",
    "        continue\n",
    "\n",
    "    final_text = \"\"\n",
    "    if fname.endswith(\"_es.txt\"):\n",
    "        print(f\"Translating Spanish file: {fname}\")\n",
    "        final_text = translate_text(text, \"Helsinki-NLP/opus-mt-es-en\")\n",
    "    elif fname.endswith(\"_bn.txt\"): \n",
    "        print(f\"Translating Bengali file: {fname}\")\n",
    "        final_text = translate_text(text, \"Helsinki-NLP/opus-mt-bn-en\")\n",
    "    else:\n",
    "        print(f\"Copying English file: {fname}\")\n",
    "        final_text = text \n",
    "    \n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(final_text)\n",
    "\n",
    "print(\"\\nTranslation and processing complete. Final English files are in 'texts_translated'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b358fc9-6598-4687-94a7-eb4d04ac976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 English .txt files to chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 26 chunks for Bangladesh_ClimateChangeTrustAct_2010_bn.txt (Country: Bangladh)\n",
      "Saved 182 chunks for Chile_LeyMarcoCambioClimatico_2022_es.txt (Country: Chile)\n",
      "Saved 161 chunks for Fiji_ClimateChangeAct_2021.txt (Country: Fiji)\n",
      "Saved 46 chunks for Kenya_ClimateChangeAct_2016.txt (Country: Kenya)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents: 100%|██████████| 12/12 [00:00<00:00, 28.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 445 chunks for NAP_Bangladesh.txt (Country: Bangladh)\n",
      "Saved 536 chunks for NAP_Chile.txt (Country: Chile)\n",
      "Saved 241 chunks for NAP_Fiji.txt (Country: Fiji)\n",
      "Saved 107 chunks for NAP_Kenya.txt (Country: Kenya)\n",
      "Saved 217 chunks for NAP_Pakistan.txt (Country: Pakistan)\n",
      "Saved 169 chunks for NAP_SouthAfrica.txt (Country: South Africa)\n",
      "Saved 22 chunks for Pakistan_ClimateChangeAct_2017.txt (Country: Pakistan)\n",
      "Saved 55 chunks for SouthAfrica_ClimateChangeBill_2022.txt (Country: South Africa)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunking complete. Manifest saved to: E:\\PROJECTS\\climate_legal_readiness_project\\data\\processed\\chunk_manifest.json\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=1200, overlap=200):\n",
    "    if not isinstance(text, str): return []\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text.strip())\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks, current_chunk = [], \"\"\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph = paragraph.strip()\n",
    "        if not paragraph: continue\n",
    "        if len(current_chunk) == 0 or len(current_chunk) + len(paragraph) + 2 <= chunk_size:\n",
    "            current_chunk += \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "        elif len(paragraph) > chunk_size:\n",
    "             if current_chunk: chunks.append(current_chunk)\n",
    "             start = 0\n",
    "             while start < len(paragraph):\n",
    "                 end, split_point = start + chunk_size, start + chunk_size\n",
    "                 if end < len(paragraph):\n",
    "                      space_pos = paragraph.rfind(' ', start + chunk_size - overlap // 2, end)\n",
    "                      if space_pos != -1: split_point = space_pos\n",
    "                 chunks.append(paragraph[start:split_point].strip())\n",
    "                 start = max(split_point - overlap, start + 1) if overlap > 0 else split_point\n",
    "             current_chunk = \"\"\n",
    "        else:\n",
    "            if current_chunk: chunks.append(current_chunk)\n",
    "            overlap_text = \"\"\n",
    "            if overlap > 0 and chunks:\n",
    "                overlap_candidate = chunks[-1][-(overlap):]\n",
    "                if len(overlap_candidate) + len(paragraph) + 2 < chunk_size + 50 :\n",
    "                    overlap_text = overlap_candidate + \"\\n\\n\"\n",
    "            current_chunk = (overlap_text + paragraph)[:chunk_size]\n",
    "    if current_chunk: chunks.append(current_chunk)\n",
    "    return [c.strip() for c in chunks if len(c.strip()) > 50]\n",
    "\n",
    "manifest = []\n",
    "text_files_to_chunk = [f for f in os.listdir(TRANSLATED_DIR) if f.endswith(\".txt\") and not f.startswith(\"_\")]\n",
    "print(f\"Found {len(text_files_to_chunk)} English .txt files to chunk.\")\n",
    "\n",
    "for fname in tqdm(text_files_to_chunk, desc=\"Chunking documents\"): \n",
    "    full_path, base_name = os.path.join(TRANSLATED_DIR, fname), os.path.splitext(fname)[0]\n",
    "    \n",
    "    parts, country_name = base_name.split('_'), \"Unknown\"\n",
    "    if len(parts) > 0:\n",
    "        if parts[0].upper() in [\"NAP\", \"LAW\", \"ACT\", \"BILL\"]:\n",
    "             if len(parts) > 1: country_name = parts[1]\n",
    "        else: country_name = parts[0]\n",
    "        country_name = country_name.replace('bn', '').replace('es', '').strip('_ ')\n",
    "        if country_name.lower() == \"southafrica\": country_name = \"South Africa\"\n",
    "        elif country_name: country_name = country_name.capitalize()\n",
    "    doc_type = \"NAP\" if \"NAP\" in base_name.upper() else \"LAW\"\n",
    "    if \"Act\" in base_name or \"Ley\" in base_name or \"Bill\" in base_name: doc_type = \"LAW\"\n",
    "    \n",
    "    try:\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: text_content = f.read()\n",
    "        if len(text_content) < 50: continue\n",
    "        list_of_chunks = chunk_text(text_content, chunk_size=1200, overlap=200)\n",
    "        if not list_of_chunks: continue\n",
    "        \n",
    "        out_jsonl_path = os.path.join(CHUNK_DIR, base_name + \".jsonl\")\n",
    "        with open(out_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "            for i, chunk_text_content in enumerate(list_of_chunks):\n",
    "                record = {\n",
    "                    \"id\": f\"{base_name}::chunk_{i}\", \"text\": chunk_text_content, \"source_file\": fname,\n",
    "                    \"country\": country_name, \"doc_type\": doc_type\n",
    "                }\n",
    "                f_out.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "        manifest.append({\"file\": fname, \"chunks\": len(list_of_chunks), \"country\": country_name, \"doc_type\": doc_type, \"status\": \"ok\"})\n",
    "        print(f\"Saved {len(list_of_chunks)} chunks for {fname} (Country: {country_name})\")\n",
    "    except Exception as e: print(f\"\\n--- Error processing file: {fname} ---: {e}\")\n",
    "\n",
    "manifest_path = os.path.join(PROC_DIR, \"chunk_manifest.json\")\n",
    "with open(manifest_path, \"w\", encoding=\"utf-8\") as f_manifest:\n",
    "    json.dump(manifest, f_manifest, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nChunking complete. Manifest saved to: {manifest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9363867-f7f0-4106-aed4-8b93c39db395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model (all-MiniLM-L6-v2)...\n",
      "Model loaded.\n",
      "Initializing vector database at: E:\\PROJECTS\\climate_legal_readiness_project\\data\\processed\\vectorstore\n",
      "Resetting collection 'climate_laws_nap'...\n",
      "Collection created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing 26 chunks for Bangladh, LAW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:   8%|▊         | 1/12 [00:02<00:22,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 26 chunks.\n",
      "\n",
      "Indexing 182 chunks for Chile, LAW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:  17%|█▋        | 2/12 [00:08<00:49,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 182 chunks.\n",
      "\n",
      "Indexing 161 chunks for Fiji, LAW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:  25%|██▌       | 3/12 [00:15<00:49,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 161 chunks.\n",
      "\n",
      "Indexing 46 chunks for Kenya, LAW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:  33%|███▎      | 4/12 [00:16<00:32,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 46 chunks.\n",
      "\n",
      "Indexing 445 chunks for Bangladh, NAP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:  42%|████▏     | 5/12 [00:35<01:03,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 445 chunks.\n",
      "\n",
      "Indexing 536 chunks for Chile, NAP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:  50%|█████     | 6/12 [00:59<01:25, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 536 chunks.\n",
      "\n",
      "Indexing 241 chunks for Fiji, NAP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:  58%|█████▊    | 7/12 [01:10<01:06, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 241 chunks.\n",
      "\n",
      "Indexing 107 chunks for Kenya, NAP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:  67%|██████▋   | 8/12 [01:16<00:43, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 107 chunks.\n",
      "\n",
      "Indexing 217 chunks for Pakistan, NAP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:  75%|███████▌  | 9/12 [01:26<00:32, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 217 chunks.\n",
      "\n",
      "Indexing 169 chunks for South Africa, NAP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:  83%|████████▎ | 10/12 [01:35<00:20, 10.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 169 chunks.\n",
      "\n",
      "Indexing 22 chunks for Pakistan, LAW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:  92%|█████████▏| 11/12 [01:36<00:07,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 22 chunks.\n",
      "\n",
      "Indexing 55 chunks for South Africa, LAW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents: 100%|██████████| 12/12 [01:39<00:00,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 55 chunks.\n",
      "\n",
      "Re-indexing complete. Total chunks indexed: 2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embedding model (all-MiniLM-L6-v2)...\")\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded.\")\n",
    "print(f\"Initializing vector database at: {VECTOR_DIR}\")\n",
    "client = chromadb.PersistentClient(path=VECTOR_DIR, settings=Settings(allow_reset=True))\n",
    "\n",
    "print(\"Resetting collection 'climate_laws_nap'...\")\n",
    "try:\n",
    "    client.delete_collection(name=\"climate_laws_nap\")\n",
    "    time.sleep(1) \n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not delete collection (may not exist): {e}\")\n",
    "\n",
    "collection = client.create_collection(name=\"climate_laws_nap\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "print(\"Collection created.\")\n",
    "\n",
    "jsonl_files = [f for f in os.listdir(CHUNK_DIR) if f.endswith(\".jsonl\")]\n",
    "total_indexed = 0\n",
    "\n",
    "for jsonl_file in tqdm(jsonl_files, desc=\"Indexing documents\"): \n",
    "    file_path = os.path.join(CHUNK_DIR, jsonl_file)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            records = [json.loads(line) for line in f if line.strip()]\n",
    "        if not records: continue\n",
    "\n",
    "        texts = [r.get(\"text\", \"\") for r in records]\n",
    "        ids = [r.get(\"id\") for r in records]\n",
    "        metadatas = [{\"source_file\": r.get(\"source_file\"), \"country\": r.get(\"country\"), \"doc_type\": r.get(\"doc_type\")} for r in records]\n",
    "        \n",
    "        valid_indices = [i for i, t in enumerate(texts) if t]\n",
    "        if len(valid_indices) < len(texts):\n",
    "            texts = [texts[i] for i in valid_indices]\n",
    "            ids = [ids[i] for i in valid_indices]\n",
    "            metadatas = [metadatas[i] for i in valid_indices]\n",
    "\n",
    "        if not texts:\n",
    "             print(f\"Skipping {jsonl_file}, no valid text found.\")\n",
    "             continue\n",
    "\n",
    "        print(f\"\\nIndexing {len(texts)} chunks for {records[0].get('country')}, {records[0].get('doc_type')}...\")\n",
    "        \n",
    "        embeddings = embedder.encode(texts, batch_size=32, show_progress_bar=False)\n",
    "        embeddings_list = embeddings.tolist()\n",
    "\n",
    "        collection.add(documents=texts, embeddings=embeddings_list, metadatas=metadatas, ids=ids)\n",
    "        total_indexed += len(records)\n",
    "        print(f\"Successfully added {len(records)} chunks.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Error processing file: {jsonl_file} ---: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\nRe-indexing complete. Total chunks indexed: {total_indexed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64fb4cfb-17a5-4980-b787-ea9d79f8d244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to Ollama server...\n",
      "Success! Connected to Ollama.\n",
      "\n",
      "Available models: ['phi3:mini', 'phi3:latest', 'llama3:latest']\n",
      " Found 'llama3' model.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Attempting to connect to Ollama server...\")\n",
    "    models_data_raw = ollama.list()\n",
    "    print(\"Success! Connected to Ollama.\")\n",
    "    if models_data_raw and models_data_raw.get('models'):\n",
    "        available_models = [m.get('model', 'Attribute Error') for m in models_data_raw['models']]\n",
    "        print(f\"\\nAvailable models: {available_models}\")\n",
    "        \n",
    "        if any('llama3' in m for m in available_models):\n",
    "            print(\" Found 'llama3' model.\")\n",
    "        else:\n",
    "            print(\" 'llama3' model not found! Please run 'ollama pull llama3' in your terminal.\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: Unexpected data structure from ollama.list(). Raw data: {models_data_raw}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n Error connecting to Ollama: {e}\")\n",
    "    print(\"Please ensure the Ollama application is running on your computer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f4b0843-24c8-4939-afdb-675103a256b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import time # Added for potential delays\n",
    "\n",
    "# --- Re-establish connections just in case kernel restarted ---\n",
    "# print(\"Loading embedding model (all-MiniLM-L6-v2)...\")\n",
    "# embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# print(\"Connecting to vector database...\")\n",
    "# client = chromadb.PersistentClient(path=VECTOR_DIR)\n",
    "# collection = client.get_collection(name=\"climate_laws_nap\")\n",
    "# print(\"Setup complete for RAG function.\")\n",
    "# # -----------------------------------------------------------\n",
    "\n",
    "def get_local_rag_answer(query: str, country_filter: str = None):\n",
    "    \"\"\"\n",
    "    Answers a query using Ollama and a local ChromaDB vectorstore.\n",
    "    - If country_filter is provided, it filters for that specific country.\n",
    "    - If country_filter is None, it searches all documents for general patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    query_embedding = embedder.encode([query])[0].tolist() \n",
    "    if country_filter:\n",
    "        where_clause = {\"country\": country_filter}\n",
    "        print(f\"Retrieving specific context for: {country_filter}...\")\n",
    "    else:\n",
    "        where_clause = None \n",
    "        print(\"Retrieving generalized context from all 6 countries...\")\n",
    "\n",
    "    try:\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=5,\n",
    "            where=where_clause, \n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying ChromaDB: {e}\")\n",
    "        return \"Error: Could not retrieve documents from the vector database.\"\n",
    "\n",
    "    if not results or not results.get(\"documents\") or not results[\"documents\"][0]:\n",
    "        print(\"Warning: No documents found matching the query criteria.\")\n",
    "        if country_filter:\n",
    "            print(\"Trying generalized search instead...\")\n",
    "            return get_local_rag_answer(query, country_filter=None)\n",
    "        else:\n",
    "             return \"Sorry, I couldn't find any relevant information for that query, even by analogy.\"\n",
    "    docs = results[\"documents\"][0]\n",
    "    metas = results[\"metadatas\"][0]\n",
    "    dists = results[\"distances\"][0]\n",
    "\n",
    "\n",
    "    context_blocks = []\n",
    "    for i, (doc, meta, dist) in enumerate(zip(docs, metas, dists), start=1):\n",
    "         if meta:\n",
    "              source_info = f\"[Source {i}: {meta.get('country', 'N/A')} ({meta.get('doc_type', 'N/A')}) | File: {meta.get('source_file', 'N/A')}]\"\n",
    "              context_blocks.append(f\"{source_info}\\n{doc}\")\n",
    "         else:\n",
    "              context_blocks.append(f\"[Source {i}: Metadata missing]\\n{doc}\")\n",
    "\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_blocks)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert global climate policy analyst. You will be given a user's question\n",
    "    and a set of text chunks from the climate laws and National Adaptation Plans (NAPs)\n",
    "    of 6 representative countries: Bangladesh (Asia), Chile (South America), Fiji (Oceania), \n",
    "    Kenya (Africa), Pakistan (Asia), South Africa (Africa).\n",
    "\n",
    "    Your task is to answer the user's question using ONLY the provided text chunks (context).\n",
    "\n",
    "    **Reasoning Process:**\n",
    "\n",
    "    1.  **Identify Target:** Determine if the question is about a specific country *within* the context (e.g., Kenya) or a country *outside* the context (e.g., India).\n",
    "    2.  **Specific Question:** If the question is about a country *within* the context (e.g., Kenya), primarily use sources from that specific country found in the context.\n",
    "    3.  **Analogical Question:** If the question is about a country *outside* the context (e.g., India):\n",
    "        * **Prioritize Region:** First, look for examples in the context from countries in the *same continent/region* as the target country (e.g., for India, prioritize context from Bangladesh and Pakistan).\n",
    "        * **Synthesize Patterns:** Combine the relevant regional examples (if any) with common patterns found across *all* provided sources to create a generalized, analogous answer. Do not just focus on one country's example.\n",
    "        * **State Analogy:** Clearly state that the answer is an analogy based on available examples.\n",
    "    4.  **Citation:** You MUST cite facts in your answer using the [Source #] tags exactly as provided in the context (e.g., [Source 1], [Source 3]). Cite *after each piece of information*.\n",
    "    5.  **Conciseness & Factuality:** Be factual, concise, and use bullet points.\n",
    "    6.  **No Information:** If the context does not contain relevant information to answer the question (even by analogy), state clearly that the information is not available in the provided documents. Do not invent information.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"Question: {query}\\n\\nContext:\\n{context}\\n\\nAnswer:\"\n",
    "\n",
    "    print(\"Sending context to Ollama (llama3)...\")\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=\"phi3:mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"--- OLLAMA ERROR ---\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Please ensure the Ollama application is running and you have pulled the 'llama3' model.\")\n",
    "        print(\"Run 'ollama pull llama3' in your terminal if needed.\")\n",
    "        time.sleep(2) \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab90dbf-678c-4f90-a77e-2e9def8af821",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_india = \"What are adaptation priorities for a country like India, based on regional examples?\"\n",
    "\n",
    "answer_india = get_local_rag_answer(query_india)\n",
    "\n",
    "if answer_india:\n",
    "    print(\"\\n--- GENERALIZED ANSWER (for India) ---\")\n",
    "    print(answer_india)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "726e3814-aeb6-49f5-aa69-566a6d145af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing retrieval for: 'What are the priority sectors for climate adaptation in Kenya?' WITH Kenya filter ---\n",
      "Found results specifically for Kenya:\n",
      "\n",
      "Result 1 (Distance: 0.2487)\n",
      "Source: NAP_Kenya.txt\n",
      "Text: Figure 11: Kenya’s Adaptation theory of change\n",
      "ADAPTATION INDICATORS\n",
      "National Sector County\n",
      "• Human development • Number of sectors • Number of counties that have\n",
      "index planning, budgeting integrated climate change\n",
      "and implementing adaptation in their CIDPs\n",
      "• Percentage of climate\n",
      "climate change\n",
      "rel...\n",
      "--------------------\n",
      "\n",
      "Result 2 (Distance: 0.2576)\n",
      "Source: NAP_Kenya.txt\n",
      "Text: Preface\n",
      "Charles Sunkuli, Principal Secretary - State Department of Environment\n",
      "This National Adaptation Plan (NAP 2015-2030) is a critical response\n",
      "to the climate change challenge facing our country. The NAP is\n",
      "Kenya’s first plan on adaptation, and demonstrates our commitment\n",
      "to operationalise the N...\n",
      "--------------------\n",
      "\n",
      "Result 3 (Distance: 0.2641)\n",
      "Source: NAP_Kenya.txt\n",
      "Text: Endnotes\n",
      "1 15th Session of the African Ministerial Conference on the Environment, 2015, Cairo Declaration on\n",
      "Managing Africa’s Natural Capital for Sustainable Development and Poverty Eradication (AMCEN).\n",
      "Accessed at: http://web.unep.org/african-ministers-call-adaptation-mitigation-parity-2015-climat...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "test_query = \"What are the priority sectors for climate adaptation in Kenya?\"\n",
    "test_query_embedding = embedder.encode([test_query])[0].tolist()\n",
    "\n",
    "print(f\"--- Testing retrieval for: '{test_query}' WITH Kenya filter ---\")\n",
    "try:\n",
    "    results_kenya = collection.query(\n",
    "        query_embeddings=[test_query_embedding],\n",
    "        n_results=3,\n",
    "        where={\"country\": \"Kenya\"},\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    if results_kenya and results_kenya.get(\"documents\") and results_kenya[\"documents\"][0]:\n",
    "        print(\"Found results specifically for Kenya:\")\n",
    "        for i, (doc, meta, dist) in enumerate(zip(results_kenya[\"documents\"][0], results_kenya[\"metadatas\"][0], results_kenya[\"distances\"][0])):\n",
    "            print(f\"\\nResult {i+1} (Distance: {dist:.4f})\")\n",
    "            print(f\"Source: {meta.get('source_file', 'N/A')}\")\n",
    "            print(f\"Text: {doc[:300]}...\") \n",
    "            print(\"-\" * 20)\n",
    "    else:\n",
    "        print(\" No results found specifically for Kenya with this query.\")\n",
    "        print(\"Raw results:\", results_kenya)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error during direct retrieval test: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e3825-57a3-4535-8ed6-05813bb5ae36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_kenya = \"What are the specific climate change adaptation sectors mentioned in Kenya's NAP?\"\n",
    "\n",
    "answer_kenya = get_local_rag_answer(query_kenya, country_filter=\"Kenya\")\n",
    "\n",
    "if answer_kenya:\n",
    "    print(\"\\n--- SPECIFIC ANSWER (for Kenya) ---\")\n",
    "    print(answer_kenya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bfce713-9021-43dd-886e-9df7163f693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pakistan | NAP | NAP_Pakistan.txt]  score=0.4164\n",
      "vel of coordination between the MoCC&EC and the provincial governments was essential, because it serves as the cornerstone of effective climate governance and implementation of the NAP countrywide. 33  34  4. Adaptation Priorities NATIONAL ADAPTATION PLANP A K I S T A N 2 0 2 3 35\n",
      "------------------------------------------------------------------------------------------\n",
      "[Chile | LAW | Chile_LeyMarcoCambioClimatico_2022_es.txt]  score=0.4169\n",
      "lopment of which shall be the responsibility of the Ministry of Agriculture; h) Fisheries and aquaculture, the development of which shall be the responsibility of the Ministry of Economy, Development, and Tourism; i) Cities, the development of which shall be the responsibility of the Ministry of Housing and Urban Development; j) Tourism, the development of which shall be the responsibility of the \n",
      "------------------------------------------------------------------------------------------\n",
      "[Chile | NAP | NAP_Chile.txt]  score=0.4178\n",
      "participation of local actors, both the government, the private sector, the academy and the citizenry in the process. Considering that the implementation of the sectoral plans requires the following:  In this sense, it will also be useful to have criteria for prioritisation that allow for more efficient and effective implementation. Prioritization criteria in the implementation of adaptation measu\n",
      "------------------------------------------------------------------------------------------\n",
      "[Chile | NAP | NAP_Chile.txt]  score=0.4197\n",
      "e important when the type of impacts or the degree of expected impact is associated with a high level of uncertainty. · Effectiveness: Adaptation measures should increase resilience to climate change.  · Feasibility: the purpose of a Sectoral Adaptation Plan is to ensure that all of its measures are implemented. However, some of them may have a greater potential to be implemented when appropriate \n",
      "------------------------------------------------------------------------------------------\n",
      "[Pakistan | NAP | NAP_Pakistan.txt]  score=0.4217\n",
      "f 2022. It involved ve steps: (i) An initial internal assessment by the MoCC&EC, in concert with the Working Group constituted to update the NDCs; (ii) stakeholder engagement in the form of assessment contributions sought through consultations with a wide range of actors, including relevant government ministries, technical experts, research institutions, and sector specialists; (iii) alignment wit\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "query = \"What are Londond's main climate adaptation priority sectors?\"\n",
    "\n",
    "q_emb = embedder.encode([query])[0]\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[q_emb],\n",
    "    n_results=5,\n",
    "    include=[\"documents\",\"metadatas\",\"distances\"],\n",
    "\n",
    ")\n",
    "\n",
    "for doc, meta, dist in zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]):\n",
    "    print(f\"[{meta['country']} | {meta['doc_type']} | {meta['source_file']}]  score={dist:.4f}\")\n",
    "    print(doc[:400].replace(\"\\n\", \" \"))\n",
    "    print(\"-\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51194097-245e-406b-9c60-531b99651d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
